{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "3_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WDdC5E0llYu",
        "outputId": "13a201c2-a5ef-4356-c02e-64b9864a89ed"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XQHea_TnP4S"
      },
      "source": [
        "# Install idx2numpy package for extracting data\n",
        "!pip install idx2numpy --quiet\n",
        "!pip install livelossplot --quiet"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9CsmgA1jCg4"
      },
      "source": [
        "# Import packages\n",
        "import os\n",
        "import json\n",
        "import gzip\n",
        "import torch\n",
        "import random\n",
        "import torchvision\n",
        "import numpy as np \n",
        "\n",
        "import idx2numpy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcyHL4VSIAvp"
      },
      "source": [
        "def get_best_results(results):\n",
        "  '''\n",
        "  Take in a results dictionary \n",
        "  and return epoch on which \n",
        "  minimum validation loss was\n",
        "  reached, minimum val. loss on\n",
        "  that epoch, and min. accuracy\n",
        "  on that epoch\n",
        "  '''\n",
        "  # Find the epoch on which the minimum validation loss was reached\n",
        "  best_val_epoch = np.argmin(np.array(results['val_loss']))\n",
        "  \n",
        "  # Store validation metrics\n",
        "  best_epoch_val_loss = np.array(results['val_loss'][best_val_epoch])\n",
        "  best_epoch_val_accuracy = np.array(results['val_accuracy'][best_val_epoch])\n",
        "\n",
        "  # Store training metrics\n",
        "  best_epoch_train_loss = np.array(results['train_loss'][best_val_epoch])\n",
        "  best_epoch_train_accuracy = np.array(results['train_accuracy'][best_val_epoch])\n",
        "  \n",
        "  # Store best results in a list\n",
        "  best_results = [np.int(best_val_epoch), \n",
        "                  np.float(best_epoch_val_loss), \n",
        "                  np.float(best_epoch_val_accuracy), \n",
        "                  np.float(best_epoch_train_loss), \n",
        "                  np.float(best_epoch_train_accuracy)]\n",
        "\n",
        "  # Return statement\n",
        "  return(best_results)\n",
        "\n",
        "\n",
        "def plot_loss(results, lab, model_num, experiment_id):\n",
        "  '''\n",
        "  Convenience function to plot results\n",
        "  '''\n",
        "  # Store model name and destination path\n",
        "  model_name = str(experiment_id) + '_' + str(model_num) + '_' + lab + '_results.png'\n",
        "  path = os.path.join('/content/drive/MyDrive/', '1_figs', model_name)\n",
        "  \n",
        "  # Plot the results\n",
        "  plt.plot(results['train_' + lab], label='Train')\n",
        "  plt.plot(results['val_' + lab], label='Validation')\n",
        "  \n",
        "  # Add annotations\n",
        "  plt.legend()\n",
        "  plt.title(lab.title() + ' by Epoch')\n",
        "  \n",
        "  # Save the figure and close the plot\n",
        "  plt.savefig(path)\n",
        "  plt.clf()"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmfcKJJCjCg5"
      },
      "source": [
        "def load_one_dataset(path):\n",
        "    '''\n",
        "    Convenience function to load a single dataset\n",
        "    '''\n",
        "    f = gzip.open(path, 'rb')\n",
        "    data = torch.from_numpy(idx2numpy.convert_from_file(f))\n",
        "    f.close()\n",
        "    \n",
        "    return(data)\n",
        "\n",
        "\n",
        "def load_all_datasets(train_imgs, train_labs, test_imgs, test_labs, batch_size, train_size=50000, val_size=10000):\n",
        "    '''\n",
        "    Load training as well as test images here\n",
        "    '''\n",
        "    torch.manual_seed(123809)\n",
        "    train_images = load_one_dataset(train_imgs).type(torch.float32)/255.0\n",
        "    train_labels = load_one_dataset(train_labs).type(torch.long)\n",
        "    train = list(zip(train_images, train_labels))\n",
        "    print(len(train))\n",
        "\n",
        "    test_images = load_one_dataset(test_imgs).type(torch.float32)/255.0\n",
        "    test_labels = load_one_dataset(test_labs).type(torch.long)\n",
        "    test = list(zip(test_images, test_labels))\n",
        "    \n",
        "    train, val = torch.utils.data.random_split(train, [train_size, val_size])\n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    \n",
        "    return(train_loader, val_loader, test_loader)"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxKAhnC9rieJ"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  \n",
        "  def __init__(self, nb_units, input_dim, output_dim):\n",
        "    '''\n",
        "    Declare the network architecture here\n",
        "    '''\n",
        "    super(Net, self).__init__()\n",
        "    \n",
        "    # Initialize a list to store layers\n",
        "    fc = []\n",
        "\n",
        "    # Add input and output dimensions to layer list\n",
        "    self.nb_units = [input_dim] + nb_units + [output_dim]\n",
        "\n",
        "    # Now compute the total no. of layers\n",
        "    self.nb_layers = len(self.nb_units)\n",
        "\n",
        "    # Now append the hidden layers\n",
        "    for i in range(1, self.nb_layers):\n",
        "      fc.append(nn.Linear(self.nb_units[i-1], self.nb_units[i]))\n",
        "    \n",
        "    # Wrap this in a module list \n",
        "    self.fc = nn.ModuleList(fc)\n",
        "    \n",
        "  \n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    Send input forward through \n",
        "    the network\n",
        "    '''\n",
        "    # Reshape 28X28 images to be 784 X 784\n",
        "    x = x.view(-1, 28*28)\n",
        "\n",
        "    # Send example through network\n",
        "    for layer in self.fc: x = F.relu(layer(x))\n",
        "    \n",
        "    # Return statement\n",
        "    return x"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGe3H8TIjCg5"
      },
      "source": [
        "def train(config, experiment_id, input_dim=784, output_dim = 10, epochs=2,\n",
        "          data_dir = '/content/drive/MyDrive/data', checkpoints_dir = \"/content/drive/MyDrive/1_checkpoints/\", \n",
        "          opt='sgd', save=0):\n",
        "    '''\n",
        "    This is the main training loop\n",
        "    '''\n",
        "    \n",
        "    # Set the seed so each experiment is reproducible\n",
        "    np.random.seed(21390)\n",
        "    torch.manual_seed(10394)\n",
        "    \n",
        "    # Set device\n",
        "    if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda\")\n",
        "    else:\n",
        "      device = torch.device(\"cpu\")\n",
        "    \n",
        "    # Set paths to datasets\n",
        "    paths = {\n",
        "        'train_imgs': os.path.join(data_dir, 'train-images-idx3-ubyte.gz'),\n",
        "        'train_labs': os.path.join(data_dir, 'train-labels-idx1-ubyte.gz'),\n",
        "        'test_imgs': os.path.join(data_dir,'t10k-images-idx3-ubyte.gz'),\n",
        "        'test_labs': os.path.join(data_dir,'t10k-labels-idx1-ubyte.gz')\n",
        "    }\n",
        "\n",
        "    # Load datasets\n",
        "    train_loader, val_loader, test_loader = load_all_datasets(**paths, batch_size = config['batch_size'])\n",
        "    \n",
        "    # Set parameters\n",
        "    net = Net(config['nb_units'], input_dim, output_dim)\n",
        "    \n",
        "    # Send net object to device memory\n",
        "    net.to(device)\n",
        "    \n",
        "    # We use the cross-entropy loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # We use mini-batch stochastic gradient descent with momentum\n",
        "    if opt == 'sgd':\n",
        "      optimizer = optim.SGD(net.parameters(), lr=config['lr'], momentum=config['momentum'], \n",
        "                            weight_decay=config['weight_decay'])\n",
        "    elif opt == 'adam':\n",
        "      optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "\n",
        "    # Store results here\n",
        "    results = {\n",
        "      'train_loss': [], \n",
        "      'train_accuracy': [],\n",
        "      'val_loss': [], \n",
        "      'val_accuracy': []\n",
        "      }\n",
        "\n",
        "    # Loop over the dataset multiple times\n",
        "    for epoch in range(epochs):  \n",
        "        \n",
        "        # Initialize running loss\n",
        "        running_loss = 0.0\n",
        "        running_accuracy = 0.0\n",
        "\n",
        "        # Initialize the validation running loss\n",
        "        val_running_loss = 0.0\n",
        "        val_running_accuracy = 0.0\n",
        "        \n",
        "        # Iterate through data now\n",
        "        for i, data in enumerate(train_loader):\n",
        "            \n",
        "            # Get the inputs: data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            \n",
        "            # Send the inputs to the memory of the device\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward\n",
        "            outputs = net(inputs)\n",
        "            \n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Backward\n",
        "            loss.backward()\n",
        "            \n",
        "            # Optimize\n",
        "            optimizer.step()\n",
        "\n",
        "            # Add to running loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Add to running accuracy\n",
        "            running_accuracy += (preds == labels).float().sum()\n",
        "        \n",
        "        # Loop through the validation data\n",
        "        for j, data in enumerate(val_loader):\n",
        "          \n",
        "          # No need to calculate gradients for validation set\n",
        "          with torch.no_grad():\n",
        "\n",
        "              # Get the data item \n",
        "              val_inputs, val_labels = data\n",
        "              val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
        "\n",
        "              # Send the data item through the network to get output\n",
        "              val_outputs = net(val_inputs)\n",
        "\n",
        "              # Compute the loss\n",
        "              val_loss = criterion(val_outputs, val_labels)\n",
        "\n",
        "              # Get predictions\n",
        "              _, val_preds = torch.max(val_outputs.data, 1)\n",
        "\n",
        "              # Add to running loss\n",
        "              val_running_loss += val_loss.item()\n",
        "\n",
        "              # Add to running accuracy\n",
        "              val_running_accuracy += (val_preds == val_labels).float().sum()\n",
        "        \n",
        "        # Rescale the training and validation perfomance metrics\n",
        "        running_loss = running_loss/len(train_loader)\n",
        "        running_accuracy = running_accuracy/(len(train_loader)*config['batch_size'])\n",
        "        \n",
        "        # Rescale the validation loss\n",
        "        val_running_loss = val_running_loss/len(val_loader)\n",
        "        val_running_accuracy = val_running_accuracy/(len(val_loader)*config['batch_size'])\n",
        "        \n",
        "        # Append to the results tracker\n",
        "        results['train_loss'].append(np.float(running_loss))\n",
        "        results['train_accuracy'].append(np.float(running_accuracy))\n",
        "        results['val_loss'].append(np.float(val_running_loss))\n",
        "        results['val_accuracy'].append(np.float(val_running_accuracy))\n",
        "\n",
        "        # Make print message format string\n",
        "        msg = \"{}, Epoch:{}, Loss:{}, Accuracy:{},\" \"\\n\"\n",
        "\n",
        "        # Print performance\n",
        "        print(msg.format(\"Training\", epoch, running_loss, running_accuracy))\n",
        "        print(msg.format(\"Validation\", epoch, val_running_loss, val_running_accuracy))\n",
        "\n",
        "        # Save\n",
        "        if save == 1:\n",
        "          torch.save(net.state_dict(), checkpoints_dir + 'best_model.pth')\n",
        "        \n",
        "    # Print message\n",
        "    print('Done training...')\n",
        "    \n",
        "    # Get the best results and print\n",
        "    best_results = get_best_results(results)\n",
        "    print(best_results)\n",
        "\n",
        "    # Plot the losses\n",
        "    plot_loss(results, lab='loss', model_num=config['model_num'], experiment_id=experiment_id)\n",
        "    plot_loss(results, lab='accuracy', model_num=config['model_num'], experiment_id=experiment_id)\n",
        "    \n",
        "    # Return statement\n",
        "    return(results, best_results)"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SykNTtWYrOv9"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "def test(data_dir = '/content/drive/MyDrive/data', \n",
        "         checkpoints_dir=\"/content/drive/MyDrive/1_checkpoints/\", \n",
        "         model_name='best_model.pth',input_dim=784, \n",
        "         output_dim=10, batch_size=512, nb_units = [479, 444]):\n",
        "  '''\n",
        "  Evaluate model on the test set\n",
        "  '''\n",
        "  # Set paths to datasets\n",
        "  paths = {\n",
        "      'train_imgs': os.path.join(data_dir, 'train-images-idx3-ubyte.gz'),\n",
        "      'train_labs': os.path.join(data_dir, 'train-labels-idx1-ubyte.gz'),\n",
        "      'test_imgs': os.path.join(data_dir,'t10k-images-idx3-ubyte.gz'),\n",
        "      'test_labs': os.path.join(data_dir,'t10k-labels-idx1-ubyte.gz')\n",
        "  }\n",
        "\n",
        "  # Load datasets\n",
        "  _, _, test_loader = load_all_datasets(**paths, batch_size = batch_size)\n",
        "\n",
        "  test_running_accuracy = 0.0\n",
        "\n",
        "  net = Net(nb_units, input_dim, output_dim)\n",
        "  net.load_state_dict(torch.load(checkpoints_dir + model_name))\n",
        "\n",
        "\n",
        "  pred_list=torch.zeros(0,dtype=torch.long, device='cpu')\n",
        "  lbl_list=torch.zeros(0,dtype=torch.long, device='cpu')\n",
        "\n",
        "  # Loop through the test data\n",
        "  for j, data in enumerate(test_loader):\n",
        "          \n",
        "      # No need to calculate gradients for test set\n",
        "      with torch.no_grad():\n",
        "\n",
        "          # Get the data item \n",
        "          test_inputs, test_labels = data\n",
        "          \n",
        "          # Send the data item through the network to get output\n",
        "          test_outputs = net(test_inputs)\n",
        "\n",
        "          # Get predictions\n",
        "          _, test_preds = torch.max(test_outputs.data, 1)\n",
        "\n",
        "          # Add to running accuracy\n",
        "          test_running_accuracy += (test_preds == test_labels).float().sum()\n",
        "\n",
        "          pred_list=torch.cat([pred_list, test_preds.view(-1).cpu()])\n",
        "          lbl_list=torch.cat([lbl_list, test_labels.view(-1).cpu()])\n",
        "  \n",
        "\n",
        "  conf_mat=confusion_matrix(lbl_list.numpy(), pred_list.numpy())\n",
        "  print(pd.DataFrame(conf_mat))\n",
        "        \n",
        "  # Rescale the test loss\n",
        "  test_running_accuracy = test_running_accuracy/(len(test_loader)*batch_size)\n",
        "\n",
        "  # Print per class accuracy\n",
        "  class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n",
        "  print(pd.DataFrame(class_accuracy).T)\n",
        "        \n",
        "  # Append to the results tracker\n",
        "  print(test_running_accuracy)"
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0ENz3hM7UZE"
      },
      "source": [
        "def plot_image(imgs):\n",
        "  '''\n",
        "  Take an image stored as a Torch\n",
        "  tensor and display it in the notebook\n",
        "  '''\n",
        "  plt.figure(figsize=(20,10))\n",
        "  columns = 5\n",
        "  \n",
        "  for i, img in enumerate(imgs):\n",
        "    plt.subplot(len(imgs) / columns + 1, columns, i + 1)\n",
        "    plt.imshow(img.cpu().numpy().reshape(28,28))"
      ],
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDw8bC1Q7Jg9"
      },
      "source": [
        "def plot_low_accuracy_classes(label, n, data_dir = '/content/drive/MyDrive/data', batch_size=6):\n",
        "  '''\n",
        "  Take an image stored as a Torch\n",
        "  tensor and display it in the notebook\n",
        "  '''\n",
        " # Set paths to datasets\n",
        "  paths = {\n",
        "      'train_imgs': os.path.join(data_dir, 'train-images-idx3-ubyte.gz'),\n",
        "      'train_labs': os.path.join(data_dir, 'train-labels-idx1-ubyte.gz'),\n",
        "      'test_imgs': os.path.join(data_dir,'t10k-images-idx3-ubyte.gz'),\n",
        "      'test_labs': os.path.join(data_dir,'t10k-labels-idx1-ubyte.gz')\n",
        "  }\n",
        "\n",
        "  # Load datasets\n",
        "  _, _, test_loader = load_all_datasets(**paths, batch_size = batch_size)\n",
        "\n",
        "  imgs_to_plot = []\n",
        "\n",
        "  # Loop through the test data\n",
        "  for j, data in enumerate(test_loader):\n",
        "\n",
        "          # Get the data item \n",
        "          test_inputs, test_labels = data\n",
        "          for img, lab in zip(test_inputs, test_labels):\n",
        "            if lab == label:\n",
        "              # Send the data item through the network to get output\n",
        "              imgs_to_plot.append(img)\n",
        "  imgs_to_plot = imgs_to_plot[:n]\n",
        "  imgs_to_plot = torch.stack(imgs_to_plot, dim=0)\n",
        "  imgs_to_plot = imgs_to_plot.view(imgs_to_plot.size(0),-1).type(torch.FloatTensor)\n",
        "  plot_image(imgs_to_plot)\n",
        "\n",
        "  return(imgs_to_plot)\n",
        "    "
      ],
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RibzX_A2wFfu"
      },
      "source": [
        "def get_grid(archs, learning_rates, decays, momentums, batch_sizes,\n",
        "             checkpoints_dir = \"/content/drive/MyDrive/1_checkpoints\"):\n",
        "  '''\n",
        "  Get a random set of hyper-parameter combinations\n",
        "  to test with for a given number of layers. \n",
        "  '''\n",
        "  \n",
        "  # Store the grid and results in this dictionary\n",
        "  experiment = {\n",
        "      \n",
        "      'params': [], \n",
        "      'best_results': []\n",
        "\n",
        "  }\n",
        "\n",
        "  # Create experiment ID\n",
        "  experiment_id = len([name for name in os.listdir(checkpoints_dir)]) + 1\n",
        "\n",
        "  # Initial model number\n",
        "  model_num = 0\n",
        "  \n",
        "  # This will generate the list of parameter combinations to search\n",
        "  for arch in archs:\n",
        "    for lr in learning_rates: \n",
        "      for d in decays: \n",
        "        for m in momentums: \n",
        "          for batch_size in batch_sizes:  \n",
        "            config = {\n",
        "                      'lr': lr,\n",
        "                      'batch_size': batch_size,\n",
        "                      'weight_decay': d,\n",
        "                      'nb_units': arch,\n",
        "                      'momentum': m,\n",
        "                      'done': 0,\n",
        "                      'model_num': model_num}\n",
        "          \n",
        "            model_num += 1\n",
        "            experiment['params'].append(config)\n",
        "\n",
        "  # This is the experiment path\n",
        "  path = os.path.join(checkpoints_dir, str(experiment_id) + '.json')\n",
        "\n",
        "  # We write the experiment dictionary as a .json file\n",
        "  with open(path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(experiment, f, ensure_ascii=False, indent=4)"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoFum6Ym6jm9"
      },
      "source": [
        "def run_grid_search(experiment_id, \n",
        "                    checkpoints_dir = \"/content/drive/MyDrive/1_checkpoints\", \n",
        "                    max_epochs=100, opt = 'sgd', save=0):\n",
        "  '''\n",
        "  Run the grid search\n",
        "  '''\n",
        "  # First we load the experiment dictionary\n",
        "  # Create the complete path\n",
        "  path = os.path.join(checkpoints_dir, str(experiment_id) + '.json')\n",
        "  \n",
        "  # Load the experiment data file\n",
        "  with open(path, 'r', encoding='utf-8') as f:\n",
        "    experiment = json.load(f)\n",
        "\n",
        "  # Loop through experiments data file\n",
        "  for i, config in enumerate(experiment['params']):\n",
        "    \n",
        "    # Print progress report\n",
        "    print(\"This is combination {} of experiment {}\".format(i, experiment_id))\n",
        "\n",
        "    # Run experiments only for those parameter combinations which have not been tested\n",
        "    if config['done'] == 0:\n",
        "      print(config)\n",
        "      results, best_results = train(config, epochs=max_epochs, experiment_id = experiment_id, opt=opt, save=save)\n",
        "      experiment['best_results'].append(best_results) \n",
        "      config['done'] = 1\n",
        "    \n",
        "    # We update the .json dictionary every 15 iterations\n",
        "    if i%2 == 0:\n",
        "      with open(path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(experiment, f, ensure_ascii=False, indent=4)"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNdxxSUfixGv"
      },
      "source": [
        "def get_experiment_results(experiment_id, checkpoints_dir = \"/content/drive/MyDrive/1_checkpoints\", param='nb_units'):\n",
        "  '''\n",
        "  Get the best results\n",
        "  '''\n",
        "  path = os.path.join(checkpoints_dir, str(experiment_id) + '.json')\n",
        "  \n",
        "  # Load the experiment data file\n",
        "  with open(path, 'r', encoding='utf-8') as f:\n",
        "    experiment = json.load(f)\n",
        "  \n",
        "  # Store the results\n",
        "  results = np.array(experiment['best_results'])\n",
        "  best_config = np.argmin(results[:,1])\n",
        "  best_results = results[best_config,:]\n",
        "\n",
        "  # Return statement\n",
        "  return(experiment['params'][best_config][param], experiment['params'][best_config]['model_num'], best_results)"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Bx_Qk5cbEQh"
      },
      "source": [
        "# Initialize grid for architecture search\n",
        "def run_architecture_search(experiment_id=2, new_experiment=0):\n",
        "\n",
        "  archs = [[512, 256, 128, 64, 32, 16], \n",
        "           [512, 256, 128, 64, 32], \n",
        "           [512, 256, 128, 64],\n",
        "           [512, 256, 128],\n",
        "           [512, 256],\n",
        "           [256, 128, 64, 32, 16], \n",
        "           [256, 128, 64, 32],\n",
        "           [256, 128, 64], \n",
        "           [256, 128],   \n",
        "           [128, 64, 32, 16], \n",
        "           [128, 64, 32],\n",
        "           [64, 32, 16], \n",
        "           [64, 32], \n",
        "           [32, 16],\n",
        "           [32]]\n",
        "\n",
        "  # Set other parameters\n",
        "  batch_size = [64]\n",
        "  learning_rates = [0.3]\n",
        "  decays = [0]\n",
        "  momentums = [0]\n",
        "  \n",
        "  # Recreate the grid if needed\n",
        "  if new_experiment == 1:\n",
        "    get_grid(archs, learning_rates, decays, momentums, batch_sizes=batch_size)\n",
        "  \n",
        "  # Run the search\n",
        "  run_grid_search(experiment_id = experiment_id, max_epochs=100)"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuRV-ai34KQB"
      },
      "source": [
        "# Initialize grid for architecture search\n",
        "def run_width_search(experiment_id=3, new_experiment=0):\n",
        "\n",
        "  # This tests: \n",
        "  # 1) Gaps of more than 1 in the powers of 2 widths\n",
        "  # 2) Low to high layer widths\n",
        "  # 3) Deviations from powers of 2 widths\n",
        "  archs = [[512, 128], \n",
        "           [512, 64],\n",
        "           [512, 32],\n",
        "           [512, 16],\n",
        "           [256, 64],\n",
        "           [256, 32],\n",
        "           [256, 16],\n",
        "           [128, 64],\n",
        "           [128, 32],\n",
        "           [128, 16],\n",
        "           [64, 16], \n",
        "           [16, 32],\n",
        "           [16, 64], \n",
        "           [16, 128],\n",
        "           [16, 256], \n",
        "           [16, 512], \n",
        "           [32, 64],\n",
        "           [32, 128],\n",
        "           [32, 256],\n",
        "           [32, 512],\n",
        "           [64, 128], \n",
        "           [64, 256],\n",
        "           [64, 512],\n",
        "           [128, 256],\n",
        "           [128, 512],\n",
        "           [256, 512], \n",
        "           [300, 200],  \n",
        "           [200, 100], \n",
        "           [403, 202], \n",
        "           [123, 86], \n",
        "           [702, 333],\n",
        "           [479, 444], \n",
        "           [333, 222], \n",
        "           [204, 52], \n",
        "           [502, 114],\n",
        "           [668, 45]]\n",
        "            \n",
        "\n",
        "  # Set other parameters\n",
        "  batch_size = [64]\n",
        "  learning_rates = [0.3]\n",
        "  decays = [0]\n",
        "  momentums = [0]\n",
        "  \n",
        "  # Recreate the grid if needed\n",
        "  if new_experiment == 1:\n",
        "    get_grid(archs, learning_rates, decays, momentums, batch_sizes=batch_size)\n",
        "  \n",
        "  # Run the search\n",
        "  run_grid_search(experiment_id = experiment_id, max_epochs=100)"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmPvhGtYCiv4"
      },
      "source": [
        "def run_decay_search(experiment_id=4, new_experiment=0):\n",
        "\n",
        "  # This tests: \n",
        "  # 1) Gaps of more than 1 in the powers of 2 widths\n",
        "  # 2) Low to high layer widths\n",
        "  # 3) Deviations from powers of 2 widths\n",
        "  archs = [[479, 444]]\n",
        "            \n",
        "\n",
        "  # Set other parameters\n",
        "  batch_size = [64]\n",
        "  learning_rates = [0.3]\n",
        "  decays = np.arange(0, 0.021, 0.001)[1:].tolist()\n",
        "  momentums = [0]\n",
        "  \n",
        "  # Recreate the grid if needed\n",
        "  if new_experiment == 1:\n",
        "    get_grid(archs, learning_rates, decays, momentums, batch_sizes=batch_size)\n",
        "  \n",
        "  # Run the search\n",
        "  run_grid_search(experiment_id = experiment_id, max_epochs=100)"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJx47CRGwC8M"
      },
      "source": [
        "def run_decay_search_2(experiment_id=5, new_experiment=0):\n",
        "\n",
        "  # This tests: \n",
        "  # 1) Gaps of more than 1 in the powers of 2 widths\n",
        "  # 2) Low to high layer widths\n",
        "  # 3) Deviations from powers of 2 widths\n",
        "  archs = [[479, 444]]\n",
        "            \n",
        "\n",
        "  # Set other parameters\n",
        "  batch_size = [64]\n",
        "  learning_rates = [0.3]\n",
        "  decays = np.arange(0, 0.0021, 0.0001)[1:].tolist()\n",
        "  momentums = [0]\n",
        "  \n",
        "  # Recreate the grid if needed\n",
        "  if new_experiment == 1:\n",
        "    get_grid(archs, learning_rates, decays, momentums, batch_size=batch_size)\n",
        "  \n",
        "  # Run the search\n",
        "  run_grid_search(experiment_id = experiment_id, max_epochs=100)"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj9kpWT-5Wux"
      },
      "source": [
        "def run_lr_search(experiment_id=6, new_experiment=0):\n",
        "\n",
        "  # This tests: \n",
        "  # 1) Gaps of more than 1 in the powers of 2 widths\n",
        "  # 2) Low to high layer widths\n",
        "  # 3) Deviations from powers of 2 widths\n",
        "  archs = [[479, 444]]\n",
        "            \n",
        "  # Set other parameters\n",
        "  batch_size = [64]\n",
        "  learning_rates = [0.1, 0.2, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "  decays = [0.0004]\n",
        "  momentums = [0]\n",
        "  \n",
        "  # Recreate the grid if needed\n",
        "  if new_experiment == 1:\n",
        "    get_grid(archs, learning_rates, decays, momentums, batch_size=batch_size)\n",
        "  \n",
        "  # Run the search\n",
        "  run_grid_search(experiment_id = experiment_id, max_epochs=100)"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSgeoi2NB2pW"
      },
      "source": [
        "def run_lr_search_2(experiment_id=7, new_experiment=0):\n",
        "\n",
        "  # This tests: \n",
        "  # 1) Gaps of more than 1 in the powers of 2 widths\n",
        "  # 2) Low to high layer widths\n",
        "  # 3) Deviations from powers of 2 widths\n",
        "  archs = [[479, 444]]\n",
        "            \n",
        "  # Set other parameters\n",
        "  batch_size = [64]\n",
        "  learning_rates = np.arange(0.21, 0.3, 0.01).tolist()\n",
        "  decays = [0.0004]\n",
        "  momentums = [0]\n",
        "  \n",
        "  # Recreate the grid if needed\n",
        "  if new_experiment == 1:\n",
        "    get_grid(archs, learning_rates, decays, momentums, batch_size=batch_size)\n",
        "  \n",
        "  # Run the search\n",
        "  run_grid_search(experiment_id = experiment_id, max_epochs=100)\n",
        "\n",
        "  "
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tjYDNns47GI"
      },
      "source": [
        "def run_momentum_search(experiment_id=8, new_experiment=0):\n",
        "\n",
        "  # This tests: \n",
        "  # 1) Gaps of more than 1 in the powers of 2 widths\n",
        "  # 2) Low to high layer widths\n",
        "  # 3) Deviations from powers of 2 widths\n",
        "  archs = [[479, 444]]\n",
        "            \n",
        "  # Set other parameters\n",
        "  batch_size = [64]\n",
        "  learning_rates = [0.22]\n",
        "  decays = [0.0004]\n",
        "  momentums = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "  \n",
        "  # Recreate the grid if needed\n",
        "  if new_experiment == 1:\n",
        "    get_grid(archs, learning_rates, decays, momentums, batch_sizes=batch_size)\n",
        "  \n",
        "  # Run the search\n",
        "  run_grid_search(experiment_id = experiment_id, max_epochs=100)"
      ],
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6FyS7hec8Mu"
      },
      "source": [
        "def run_batch_size_search(experiment_id=9, new_experiment=0):\n",
        "\n",
        "  # This tests: \n",
        "  # 1) Gaps of more than 1 in the powers of 2 widths\n",
        "  # 2) Low to high layer widths\n",
        "  # 3) Deviations from powers of 2 widths\n",
        "  archs = [[479, 444]]\n",
        "            \n",
        "  # Set other parameters\n",
        "  batch_size = [32, 64, 128, 256, 512]\n",
        "  learning_rates = [0.22]\n",
        "  decays = [0.0004]\n",
        "  momentums = [0.1]\n",
        "  \n",
        "  # Recreate the grid if needed\n",
        "  if new_experiment == 1:\n",
        "    get_grid(archs, learning_rates, decays, momentums, batch_sizes=batch_size)\n",
        "  \n",
        "  # Run the search\n",
        "  run_grid_search(experiment_id = experiment_id, max_epochs=80)"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhSeCImzPgqh"
      },
      "source": [
        "def run_adam_search(experiment_id=11, new_experiment=0):\n",
        "\n",
        "  # This tests: \n",
        "  # 1) Gaps of more than 1 in the powers of 2 widths\n",
        "  # 2) Low to high layer widths\n",
        "  # 3) Deviations from powers of 2 widths\n",
        "  archs = [[479, 444]]\n",
        "            \n",
        "  # Set other parameters\n",
        "  batch_size = [512]\n",
        "  learning_rates = np.arange(0.01, 0.1, 0.01).tolist()\n",
        "  decays = [0.0004]\n",
        "  momentums = [0.1]\n",
        "  \n",
        "  # Recreate the grid if needed\n",
        "  if new_experiment == 1:\n",
        "    get_grid(archs, learning_rates, decays, momentums, batch_sizes=batch_size)\n",
        "  \n",
        "  # Run the search\n",
        "  run_grid_search(experiment_id = experiment_id, max_epochs=100, opt = 'adam')"
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X-Zf_zRE5cP"
      },
      "source": [
        "def run_best_model(experiment_id = 12, new_experiment=1):\n",
        "\n",
        "  # \n",
        "  archs = [[479, 444]]\n",
        "            \n",
        "  # Set other parameters\n",
        "  batch_size = [512]\n",
        "  learning_rates = [0.22]\n",
        "  decays = [0.0004]\n",
        "  momentums = [0.1]\n",
        "\n",
        "  # \n",
        "  if new_experiment == 1:\n",
        "    get_grid(archs, learning_rates, decays, momentums, batch_sizes=batch_size)\n",
        "  \n",
        "  # Run the search\n",
        "  run_grid_search(experiment_id = experiment_id, max_epochs=85, save=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS88jdkGPdFp"
      },
      "source": [
        "# Run architecture search\n",
        "def full_pipeline(new_experiment=0):\n",
        "  \n",
        "  run_architecture_search(new_experiment)\n",
        "  \n",
        "  results = get_experiment_results(experiment_id=2)\n",
        "  print(results)\n",
        "  \n",
        "  run_width_search(new_experiment)\n",
        "  results = get_experiment_results(experiment_id=3)\n",
        "  \n",
        "  print(results)\n",
        "  run_decay_search(experiment_id=4, new_experiment=0)\n",
        "  \n",
        "  results = get_experiment_results(experiment_id=1, param='weight_decay')\n",
        "  run_decay_search_2(experiment_id=5, new_experiment=0)\n",
        "  \n",
        "  run_lr_search(experiment_id=6, new_experiment=0)\n",
        "  run_lr_search_2(new_experiment=1)\n",
        "  get_experiment_results(experiment_id=7, param='lr')\n",
        "  \n",
        "  run_momentum_search(experiment_id = 8, new_experiment=0)\n",
        "  get_experiment_results(experiment_id=8, param='momentum')\n",
        "  \n",
        "  run_batch_size_search(new_experiment=0)\n",
        "  get_experiment_results(experiment_id = 9, param = 'batch_size')\n",
        "  \n",
        "  run_adam_search(experiment_id=11, new_experiment=1)\n",
        "  run_best_model(experiment_id=12, new_experiment=1)"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL4dCO2NTXw5"
      },
      "source": [
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_I2gLOFEvNK"
      },
      "source": [
        "plot_low_accuracy_classes(label = 1, n = 10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}